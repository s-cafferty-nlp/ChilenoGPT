{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq0fzc5-tjc5"
      },
      "source": [
        "# Finetuning GPT on Chilean Spanish\n",
        "\n",
        "This is an exercise in using Huggingface to finetune GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qsdsiivS_XTy"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, GPT2LMHeadModel, pipeline, \\\n",
        "                         Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaRy6vXX_esR"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNgzva_y_1IS"
      },
      "outputs": [],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P5rp6ydf_1L7"
      },
      "outputs": [],
      "source": [
        "chileno_data = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='clean_chilean_reddit.txt',  # Chilean reddit\n",
        "    block_size=32  # length of each chunk of text to use as a datapoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV8CChkD_1P2",
        "outputId": "e5b61ed3-7d2b-4775-ea35-b32b5959cd82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([30562,   660,  7252,  8045,    11, 12940,   660,  7252,   283,  2049,\n",
              "           257,   555, 49027, 21872, 12520,   236,   114,   198, 15681,   284,\n",
              "          1326,   362, 19643,   320,  7063,   844,   295,   274,   269,  4763,\n",
              "           807,  3076]),\n",
              " torch.Size([32]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chileno_data[0], chileno_data[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qKRuSLK4_ust"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,  # MLM is Masked Language Modelling\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTUhdvZ4AFpM",
        "outputId": "36e8cd50-e5be-4a79-884a-b1c6c0493bfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[38101, 17797,  4818,   418, 50257, 50257, 50257],\n",
              "        [   51,  4131,    72, 35942, 17797,  4818,   418]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[38101, 17797,  4818,   418,  -100,  -100,  -100],\n",
              "        [   51,  4131,    72, 35942, 17797,  4818,   418]])}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collator_example = data_collator([tokenizer('Yo soy datos'), tokenizer('Tambi√©n soy datos')])\n",
        "\n",
        "collator_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOA80D7ZAG6C"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = 'gpt2'\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_checkpoint)  # load up a GPT2 model\n",
        "\n",
        "pretrained_generator = pipeline(\n",
        "    'text-generation', model=model, tokenizer='gpt2',\n",
        "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFFVcdITAZEM"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"caffsean/chilenoGPT\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=10, # number of training epochs\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
        "    warmup_steps=len(chileno_data.examples) // 5, # number of warmup steps for learning rate scheduler,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    hub_token='',\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=chileno_data.examples[:int(len(chileno_data.examples)*.8)],\n",
        "    eval_dataset=chileno_data.examples[int(len(chileno_data.examples)*.8):]\n",
        ")\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        },
        "id": "b4UqEdroAgPf",
        "outputId": "8a06541b-5966-4bae-ac1d-2998e29397d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36316' max='38020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36316/38020 3:10:38 < 08:56, 3.17 it/s, Epoch 9.55/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.498500</td>\n",
              "      <td>4.310601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.106300</td>\n",
              "      <td>3.979791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.879700</td>\n",
              "      <td>3.788628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.755400</td>\n",
              "      <td>3.664512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.616000</td>\n",
              "      <td>3.579157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.534000</td>\n",
              "      <td>3.515174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.463100</td>\n",
              "      <td>3.463155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.386700</td>\n",
              "      <td>3.432958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.278100</td>\n",
              "      <td>3.397545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1902' max='951' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [951/951 20:41]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38020' max='38020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38020/38020 3:20:42, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.498500</td>\n",
              "      <td>4.310601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.106300</td>\n",
              "      <td>3.979791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.879700</td>\n",
              "      <td>3.788628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.755400</td>\n",
              "      <td>3.664512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.616000</td>\n",
              "      <td>3.579157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.534000</td>\n",
              "      <td>3.515174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.463100</td>\n",
              "      <td>3.463155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.386700</td>\n",
              "      <td>3.432958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.278100</td>\n",
              "      <td>3.397545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.207400</td>\n",
              "      <td>3.392071</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=38020, training_loss=3.7507751234075637, metrics={'train_runtime': 12042.3252, 'train_samples_per_second': 101.025, 'train_steps_per_second': 3.157, 'total_flos': 1.986766626816e+16, 'train_loss': 3.7507751234075637, 'epoch': 10.0})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XW3rJAWBDdX"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
